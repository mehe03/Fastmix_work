DZ <- Des[, c(1, random + 1)]
dz <- as.data.frame(DZ)
colnames(dz)[1] <- "ID"
# Number of subjects and arrays per subject
m <- length(unique(Des[,1]))
ID <- sort(unique(Des[,1]))
n <- N / m
# Index for each subject
number <- dz %>%
dplyr::group_by(ID) %>%
dplyr::summarize(num = dplyr::n(), .groups = "drop") %>%
dplyr::mutate(start = dplyr::lag(cumsum(num), default = 0),
end = cumsum(num))
number <- cumsum(number$num)
number <- c(0, number)
#===========================
# Step 1: Initial estimation
#===========================
# Initial fixed effects (OLS)
coef.fix <- lm.fit(x = as.matrix(Des[, -1]), y = Y)$coefficients
resid_global <- Y - as.matrix(Des[, -1]) %*% coef.fix
sigma2_init <- as.numeric(var(resid_global))
p_rand <- length(random)
Sigma_gamma_init <- diag(0.1, p_rand)
#===========================
# Step 2: Iterative WLS + Random effects update
#===========================
iter <-100
for(i in 1:iter) {
cat("Iteration ",i, "\n\n\n")
# Precompute subject indices
idx_list <- lapply(seq_len(m), function(i) {
idx_start <- number[i] + 1
idx_end <- number[i + 1]
idx_start:idx_end
})
# Function to compute subject-specific quantities
subject_fun <- function(i) {
idx <- idx_list[[i]]
Yi <- Y[idx]
Xi <- as.matrix(Des[idx, -1])
Zi <- as.matrix(DZ[idx, -1])
# Sigma_Yi
Sigma_Yi <- Zi %*% Sigma_gamma_init %*% t(Zi) + sigma2_init * diag(nrow(Zi))
# Ui and Sigma_Ui
small_sigma <- sigma2_init
#U_i <- solve(t(Zi) %*% Zi + small_sigma * diag(ncol(Zi))) %*% t(Zi)
#Sigma_Ui <- U_i %*% Sigma_Yi %*% t(U_i)
# Components for fixed effects
p1 <- t(Xi) %*% solve(Sigma_Yi) %*% Xi
p2 <- t(Xi) %*% solve(Sigma_Yi) %*% Yi
list(
Sigma_Yi = Sigma_Yi,
#U_i = U_i,
#Sigma_Ui = Sigma_Ui,
p1 = p1,
p2 = p2,
idx = idx,
Zi = Zi
)
}
# Apply to all subjects
subject_results <- lapply(seq_len(m), subject_fun)
# Sum p1 and p2 for WLS
p1_list <- Reduce("+", lapply(subject_results, `[[`, "p1"))
p2_list <- Reduce("+", lapply(subject_results, `[[`, "p2"))
# WLS estimate of fixed effects
beta_wls <- solve(p1_list) %*% p2_list
# Residuals for gamma update
resid_wls <- Y - as.matrix(Des[, -1]) %*% beta_wls
# Update random effects gamma
gamma_results <- lapply(seq_len(m), function(i) {
Zi <- subject_results[[i]]$Zi
idx <- subject_results[[i]]$idx
#Sigma_Ui <- subject_results[[i]]$Sigma_Ui
Sigma_Yi <- subject_results[[i]]$Sigma_Yi
# Woodbury identity pieces
wood_1 <- rsolve(rsolve(Sigma_gamma_init) + (1 / sigma2_init) * (t(Zi) %*% Zi), min.cond.num = 1e-6)
wood_2 <- (1 / sigma2_init^2) * Zi %*% wood_1 %*% t(Zi)
Vi_inv <- (1 / sigma2_init) * diag(nrow(Zi)) - wood_2
Ui = Sigma_gamma_init %*% t(Zi) %*% Vi_inv
Sigma_Ui = Sigma_gamma_init - Ui%*%Zi%*%Sigma_gamma_init
# Random effects (posterior mode)
gamma_i <- Sigma_gamma_init %*% t(Zi) %*% Vi_inv %*% resid_wls[idx]
gamma_cov_i <- gamma_i %*% t(gamma_i) + Sigma_Ui
list(gamma_i = gamma_i, gamma_cov_i = gamma_cov_i)
})
# Average covariance of gamma
gamma_i_list <- lapply(gamma_results, `[[`, "gamma_i")
gamma_avg <- Reduce("+", lapply(gamma_results, `[[`, "gamma_cov_i")) / m
# Update sigma^2
ssq <- 0
for (i in 1:m) {
Xi <- as.matrix(Des[idx_list[[i]], -1])
Yi <- Y[idx_list[[i]]]
Zi <- as.matrix(DZ[idx_list[[i]], -1])
g <- gamma_i_list[[i]]
resid_post <- Yi - Xi %*% beta_wls - Zi %*% g
ssq <- ssq + sum(resid_post^2)
}
tol = Sigma_gamma_init-gamma_avg
old_sigma = Sigma_gamma_init
print(beta_wls)
print(norm(beta_wls - c(1,2,-1,2)))
print(norm(tol))
sigma2_init <- ssq / N
Sigma_gamma_init <- gamma_avg
}
beta_wls
norm(t(t(summary(lmer_model)$coef[,1]))-c(2,1,2,-1))
print(norm(beta_wls - c(1,2,-1,2)))
print(norm(beta_wls - c(1,2,-1,2)))
norm(t(t(summary(lmer_model)$coef[,1]))-c(2,1,2,-1))
print(norm(beta_wls - c(1,2,-1,2)))
beta_wls
run_fastmix_on_dataset <- function(df, RAND_LIST = c(1,2,3,4), iter = 50) {
df$X00 <- 1
Des <- df[, c(1, 4, 5, 6, 7)]
y_values <- df[3]
Y <- as.matrix(y_values)
random <- RAND_LIST
out <- fastmix_iterative(
Y = Y,
Des = Des,
random = random,
iter = iter
)
return(out)
}
library(progressr)
# Start timer
start_time <- Sys.time()
print(start_time)
with_progress({
p <- progressor(along = sim_list)
results_list <- future_lapply(sim_list, function(df) {
p()                              # advance progress bar
run_fastmix_on_dataset(df)       # run your function
})
})
fastmix_iterative <- function(Y, Des, random = "all", iter = 100) {
N <- length(Y)
covariates <- colnames(Des)[-1]
p <- length(covariates)
if(length(random) == 1 && random == "all") {
random <- 1:p
}
DZ <- Des[, c(1, random + 1)]
dz <- as.data.frame(DZ)
colnames(dz)[1] <- "ID"
m <- length(unique(Des[,1]))
ID <- sort(unique(Des[,1]))
n <- N / m
# Indexing per subject
number <- dz %>%
dplyr::group_by(ID) %>%
dplyr::summarize(num = dplyr::n(), .groups = "drop") %>%
dplyr::mutate(start = dplyr::lag(cumsum(num), default = 0),
end = cumsum(num))
number <- cumsum(number$num)
number <- c(0, number)
idx_list <- lapply(seq_len(m), function(i) {
(number[i] + 1):number[i + 1]
})
# --- Initial estimates ---
coef.fix <- lm.fit(x = as.matrix(Des[, -1]), y = Y)$coefficients
resid_global <- Y - as.matrix(Des[, -1]) %*% coef.fix
sigma2_init <- as.numeric(var(resid_global))
p_rand <- length(random)
Sigma_gamma_init <- diag(0.1, p_rand)
# Storage across iterations
beta_history <- vector("list", iter)
Sigma_gamma_history <- vector("list", iter)
gamma_history <- vector("list", iter)
sigma2_history <- vector("list",iter)
for(it in 1:iter) {
# For diagnostics
#cat("Iteration:", it, "\n")
# Subject-specific function
subject_fun <- function(i) {
idx <- idx_list[[i]]
Yi <- Y[idx]
Xi <- as.matrix(Des[idx, -1])
Zi <- as.matrix(DZ[idx, -1])
Sigma_Yi <- Zi %*% Sigma_gamma_init %*% t(Zi) + sigma2_init * diag(nrow(Zi))
calc_once = solve(Sigma_Yi)
p1 <- t(Xi) %*% calc_once %*% Xi
p2 <- t(Xi) %*% calc_once %*% Yi
list(Sigma_Yi = Sigma_Yi, p1 = p1, p2 = p2, Xi = Xi, Zi = Zi, Yi = Yi, idx = idx)
}
subject_results <- lapply(seq_len(m), subject_fun)
# Weighted least squares
p1_sum <- Reduce("+", lapply(subject_results, `[[`, "p1"))
p2_sum <- Reduce("+", lapply(subject_results, `[[`, "p2"))
beta_wls <- solve(p1_sum) %*% p2_sum
# Random effects estimation
gamma_out <- lapply(seq_len(m), function(i) {
Zi <- subject_results[[i]]$Zi
idx <- subject_results[[i]]$idx
Sigma_Yi <- subject_results[[i]]$Sigma_Yi
resid_i <- Y[idx] - as.matrix(Des[idx, -1]) %*% beta_wls
# Woodbury
wood_1 <- rsolve(rsolve(Sigma_gamma_init) + (1 / sigma2_init) * (t(Zi) %*% Zi),
min.cond.num = 1e-6)
wood_2 <- (1 / sigma2_init^2) * Zi %*% wood_1 %*% t(Zi)
Vi_inv <- (1 / sigma2_init) * diag(nrow(Zi)) - wood_2
Ui <- Sigma_gamma_init %*% t(Zi) %*% Vi_inv
Sigma_Ui <- Sigma_gamma_init - Ui %*% Zi %*% Sigma_gamma_init
gamma_i <- Ui %*% resid_i
gamma_cov_i <- gamma_i %*% t(gamma_i) + Sigma_Ui
list(gamma_i = gamma_i, gamma_cov_i = gamma_cov_i)
})
# Update Σγ
Sigma_gamma_new <- Reduce("+", lapply(gamma_out, `[[`, "gamma_cov_i")) / m
# Update σ²
ssq <- 0
for(i in 1:m) {
Xi <- subject_results[[i]]$Xi
Yi <- subject_results[[i]]$Yi
Zi <- subject_results[[i]]$Zi
g  <- gamma_out[[i]]$gamma_i
resid_post <- Yi - Xi %*% beta_wls - Zi %*% g
ssq <- ssq + sum(resid_post^2)
}
sigma2_new <- ssq / N
# --- Store iteration results ---
beta_history[[it]] <- beta_wls
Sigma_gamma_history[[it]] <- Sigma_gamma_new
gamma_history[[it]] <- lapply(gamma_out, `[[`, "gamma_i")
sigma2_history[[it]] = sigma2_new
# Update for next iteration
Sigma_gamma_init <- Sigma_gamma_new
sigma2_init <- sigma2_new
}
# Return all history
return(list(
beta_history = beta_history,
Sigma_gamma_history = Sigma_gamma_history,
gamma_history = gamma_history,
sigma2_history = sigma2_history
))
}
run_fastmix_on_dataset <- function(df, RAND_LIST = c(1,2,3,4), iter = 50) {
df$X00 <- 1
Des <- df[, c(1, 4, 5, 6, 7)]
y_values <- df[3]
Y <- as.matrix(y_values)
random <- RAND_LIST
out <- fastmix_iterative(
Y = Y,
Des = Des,
random = random,
iter = iter
)
return(out)
}
library(progressr)
# Start timer
start_time <- Sys.time()
print(start_time)
with_progress({
p <- progressor(along = sim_list)
results_list <- future_lapply(sim_list, function(df) {
p()                              # advance progress bar
run_fastmix_on_dataset(df)       # run your function
})
})
# End timer
end_time <- Sys.time()
# Time elapsed
elapsed <- end_time - start_time
print(elapsed)
saveRDS(results_list, "RDS_RESULTS.RDS")
future::nbrOfWorkers()
parallel::detectCores()
run_lmer_on_dataset <- function(df) {
# NOTE: Your df must contain Y, X1, X2, X3, and subject columns.
# If Y was previously matrix, convert to numeric:
df$Y <- as.numeric(df$Y)
lmer_model <- lmer(
Y ~ 1 + X1 + X2 + X3 + (1 + X1 + X2 + X3 | subject),
data = df,
REML = TRUE
)
return(lmer_model)
}
plan(multisession, workers = parallel::detectCores() - 1)
start_time <- Sys.time()
print(start_time)
with_progress({
p <- progressor(along = sim_list)
lmer_results_list <- future_lapply(sim_list, function(df) {
p()
run_lmer_on_dataset(df)
})
})
end_time <- Sys.time()
time_needed = end_time-start_time
print(time_needed)
run_lmer_on_dataset <- function(df) {
# NOTE: Your df must contain Y, X1, X2, X3, and subject columns.
# If Y was previously matrix, convert to numeric:
df$Y <- as.numeric(df$Y)
lmer_model <- lmer(
Y ~ 1 + X1 + X2 + X3 + (1 + X1 + X2 + X3 | subject),
data = df,
REML = TRUE
)
return(lmer_model)
}
plan(multisession, workers = parallel::detectCores() - 1)
start_time <- Sys.time()
print(start_time)
with_progress({
p <- progressor(along = sim_list)
lmer_results_list <- future_lapply(sim_list, function(df) {
p()
run_lmer_on_dataset(df)
})
})
end_time <- Sys.time()
time_needed = end_time-start_time
print(time_needed)
run_fastmix_on_dataset <- function(df, RAND_LIST = c(1,2,3,4), iter = 50) {
df$X00 <- 1
Des <- df[, c(1, 4, 5, 6, 7)]
y_values <- df[3]
Y <- as.matrix(y_values)
random <- RAND_LIST
out <- fastmix_iterative(
Y = Y,
Des = Des,
random = random,
iter = iter
)
return(out)
}
library(progressr)
# Start timer
start_time <- Sys.time()
print(start_time)
with_progress({
p <- progressor(along = sim_list)
results_list <- future_lapply(sim_list, function(df) {
p()                              # advance progress bar
run_fastmix_on_dataset(df)       # run your function
})
})
# End timer
end_time <- Sys.time()
# Time elapsed
elapsed <- end_time - start_time
print(elapsed)
saveRDS(results_list, "RDS_RESULTS.RDS")
future::nbrOfWorkers()
future::nbrOfWorkers()
# ------------------------------
# 1. True beta vector
# ------------------------------
true_beta <- c(1, 2, -1, 2)
# ------------------------------
# 2. Function to compute MSE per iteration
# ------------------------------
compute_error_path <- function(beta_history, true_beta) {
sapply(beta_history, function(b) {
b <- as.numeric(b[,1])   # convert 4x1 matrix to vector
mean((b - true_beta)^2)# MSE
#sqrt(sum((b - true_beta)^2))# norm
# To use Euclidean norm instead, uncomment the line below:
# sqrt(sum((b - true_beta)^2))
})
}
# ------------------------------
# 3. Compute MSE trajectories for all datasets
# ------------------------------
error_list <- lapply(results_list, function(res) {
compute_error_path(res$beta_history, true_beta)
})
# ------------------------------
# 4. Plotting all trajectories
# ------------------------------
# Start with first dataset to create plot frame
plot(error_list[[1]], type = "l", lwd = 1.5,
xlab = "Iteration", ylab = "MSE",
ylim = range(unlist(error_list)),  # ensure all lines fit
main = "Convergence of β Estimates Across Datasets")
# Overlay remaining datasets with semi-transparent lines
for(i in 2:length(error_list)) {
lines(error_list[[i]], col = rgb(0,0,0,0.2))
}
# ------------------------------
# 5. Plot average MSE across datasets
# ------------------------------
avg_err <- Reduce("+", error_list) / length(error_list)
lines(avg_err, col = "red", lwd = 3)
# Optional: add legend
legend("topright", legend = c("Individual datasets", "Average"),
col = c(rgb(0,0,0,0.2), "red"), lwd = c(1.5,3))
# True Σγ from DGP
random_intercept_sd <- 1
random_slope_sd <- c(0.25, 0.75, 0.5)
Sigma_gamma_true <- diag(c(random_intercept_sd^2, random_slope_sd^2))
# Function to compute Frobenius norm errors over iterations
compute_sigma_gamma_error <- function(Sigma_gamma_history, Sigma_true) {
sapply(Sigma_gamma_history, function(Sigma_est) {
#sqrt(sum((Sigma_est - Sigma_true)^2))  # Frobenius norm
mean((Sigma_est - Sigma_true)^2)      # optional: MSE instead
})
}
# Compute for all datasets
sigma_gamma_error_list <- lapply(results_list, function(res) {
compute_sigma_gamma_error(res$Sigma_gamma_history, Sigma_gamma_true)
})
# Plot first dataset to create frame
plot(sigma_gamma_error_list[[1]], type = "l", lwd = 1.5,
xlab = "Iteration", ylab = "MSE Err",
ylim = range(unlist(sigma_gamma_error_list)),
main = "Convergence of Σγ Estimates Across Datasets")
# Overlay remaining datasets
for(i in 2:length(sigma_gamma_error_list)) {
lines(sigma_gamma_error_list[[i]], col = rgb(0,0,0,0.2))
}
# Average error across datasets
avg_sigma_err <- Reduce("+", sigma_gamma_error_list) / length(sigma_gamma_error_list)
lines(avg_sigma_err, col = "red", lwd = 3)
# Optional legend
legend("topright", legend = c("Individual datasets", "Average"),
col = c(rgb(0,0,0,0.2), "red"), lwd = c(1.5,3))
# ------------------------------
# 1. Function to compute average error per iteration for dataset
# ------------------------------
compute_avg_subject_gamma_error <- function(gamma_history, true_re_df) {
# gamma_history: list of iterations, each element is list of subject vectors
# true_re_df: data frame with columns: rand_intercept, b_X1, b_X2, b_X3
n_iter <- length(gamma_history)
n_subjects <- nrow(true_re_df)
sapply(1:n_iter, function(it) {
# For each subject, compute squared error
errors <- sapply(1:n_subjects, function(subj_idx) {
gamma_est <- as.numeric(gamma_history[[it]][[subj_idx]])
gamma_true <- as.numeric(true_re_df[subj_idx, -1])  # drop subject column
mean((gamma_est - gamma_true)^2)  # MSE per subject
#sqrt(sum((gamma_est - gamma_true)^2))  #norm
})
mean(errors)  # average over all subjects
})
}
# ------------------------------
# 2. Compute average errors for all datasets
# ------------------------------
gamma_error_list <- lapply(1:length(results_list), function(i) {
compute_avg_subject_gamma_error(
gamma_history = results_list[[i]]$gamma_history,
true_re_df = simple_re_list[[i]]
)
})
# ------------------------------
# 2. Compute average errors for all datasets
# ------------------------------
plan(multisession, workers = parallel::detectCores() - 1)
gamma_error_list <- lapply(1:length(results_list), function(i) {
compute_avg_subject_gamma_error(
gamma_history = results_list[[i]]$gamma_history,
true_re_df = simple_re_list[[i]]
)
})
# ------------------------------
# 1. Function to compute average error per iteration for dataset
# ------------------------------
compute_avg_subject_gamma_error <- function(gamma_history, true_re_df) {
# gamma_history: list of iterations, each element is list of subject vectors
# true_re_df: data frame with columns: rand_intercept, b_X1, b_X2, b_X3
n_iter <- length(gamma_history)
n_subjects <- nrow(true_re_df)
sapply(1:n_iter, function(it) {
# For each subject, compute squared error
errors <- sapply(1:n_subjects, function(subj_idx) {
gamma_est <- as.numeric(gamma_history[[it]][[subj_idx]])
gamma_true <- as.numeric(true_re_df[subj_idx, -1])  # drop subject column
mean((gamma_est - gamma_true)^2)  # MSE per subject
#sqrt(sum((gamma_est - gamma_true)^2))  #norm
})
mean(errors)  # average over all subjects
})
}
# ------------------------------
# 2. Compute average errors for all datasets
# ------------------------------
gamma_error_list <- lapply(1:length(results_list), function(i) {
compute_avg_subject_gamma_error(
gamma_history = results_list[[i]]$gamma_history,
true_re_df = simple_re_list[[i]]
)
})
# ------------------------------
# 3. Plotting all datasets together
# ------------------------------
# Plot first dataset to create plot frame
plot(gamma_error_list[[1]], type = "l", lwd = 1.5,
xlab = "Iteration", ylab = "Average MSE per subject",
ylim = range(unlist(gamma_error_list)),
main = "Convergence of γ Estimates Across Datasets")
# Overlay remaining datasets with semi-transparent lines
for(i in 2:length(gamma_error_list)) {
lines(gamma_error_list[[i]], col = rgb(0,0,0,0.2))
}
# Average across datasets
avg_gamma_err <- Reduce("+", gamma_error_list) / length(gamma_error_list)
lines(avg_gamma_err, col = "red", lwd = 3)
# Add legend
legend("topright", legend = c("Individual datasets", "Average"),
col = c(rgb(0,0,0,0.2), "red"), lwd = c(1.5, 3))
